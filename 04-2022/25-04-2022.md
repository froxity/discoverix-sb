# Log #25-04-2022 DSB

> Date: 25 April 2022  
> Author: Ahmad Afiq Azmi

---

<br>

# What I Learn üìö

<br>

## Microservices

<br>

---

<br>

## Docker üê≥

<br>

### Managing Data & Working with Volumes

- We saw, that anonymous volumes are **removed automatically**, when a container is removed.

- This happens when you start / run a container with the `--rm` option.

- If you start a container **without that option**, the anonymous volume would **NOT be removed**, even if you remove the container (with `docker rm ...`).

- Still, if you then re-create and re-run the container (i.e. you run `docker run ...` again), a **new anonymous volume will be created**. So even though the anonymous volume wasn't removed automatically, it'll also **not be helpful** because a different anonymous volume is attached the next time the container starts (i.e. you removed the old container and run a new one).

- Now you just start **piling up a bunch of unused anonymous volumes** - you can **clear them** via `docker volume rm VOL_NAME` or `docker volume prune`.

- **Bind mounts shortcut**

  - Just a quick note: If you don't always want to copy and use the full path, you can use these shortcuts:
    - macOS / Linux: `-v $(pwd):/app`
    - Windows: `-v "%cd%":/app`

- **Adding more to the .dockerignore File**

  - You can add more "to-be-ignored" files and folders to your `.dockerignore` file.

  - For example, consider adding the following to entries:
  - ```
    Dockerfile
    .git
    ```
  - This would ignore the `Dockerfile` itself as well as a potentially existing `.git` folder (if you are using Git in your project).

  - In general, you want to add anything which isn't required by your application to execute correctly.

- **Environment Variables & Security**

  - One important note about **environment variables and security**: Depending on which kind of data you're storing in your environment variables, you might not want to include the secure data directly in your `Dockerfile`.

  - Instead, go for a separate environment variables file which is then only used at runtime (i.e. when you run your container with `docker run`).

  - Otherwise, the values are "baked into the image" and everyone can read these values via `docker history <image>`.

  - For some values, this might not matter but for credentials, private keys etc. you definitely want to avoid that!

  - If you use a separate file, the values are not part of the image since you point at that file when you run `docker run`. But make sure you don't commit that separate file as part of your source control repository, if you're using source control.

<br>

### Cheat Sheet Data Volumes

<br>

#### Data & Volumes

**Images are read-only** - once they're created, they can't change (you have to rebuild them to
update them).

**Containers on the other hand can read and write** - they add a thin "read-write layer" on top
of the image. That means that they can make changes to the files and folders in the image without
actually changing the image.

But even with read-write Containers, **two big problems** occur in many applications using Docker:

1. **Data written in a Container doesn't persist**: If the Container is stopped and removed, all
   data written in the Container is lost
2. **The container Container can't interact with the host filesystem**: If you change something
   in your host project folder, those changes are not reflected in the running container. You
   need to rebuild the image (which copies the folders) and start a new container

**Problem 1** can be solved with a Docker feature called **"Volumes"**.

**Problem 2** can be solved by using **"Bind Mounts"**.

<br>

#### Volumes

Volumes are folders (and files) managed on your host machine which are connected to folders / files inside of a container.

There are **two types of Volumes**:

- **Anonymous Volumes**: Created via `-v /some/path/in/container` and **removed automatically** when a container is removed because of `--rm` added on the `docker run`
  command
- **Named Volumes**: Created via `-v some-name:/some/path/in/container` and **NOT removed automatically**

With Volumes, **data can be passed into a container** (if the folder on the host machine is not empty) and it can be saved when written by a container (changes made by the container are reflected on your host machine).

**Volumes are created and managed by Docker** - as a developer, you don't necessarily know where exactly the folders are stored on your host machine. Because the data stored in there is **not meant to be viewed or edited by you** - use "Bind Mounts" if you need to do that!

Instead, especially **Named Volumes** can help you with **persisting data**.

Since data is not just written in the container but also on your host machine, the **data survives even if a container is removed** (because the Named Volume isn't removed in that case). Hence you can use Named Volumes to persist container data (e.g. log files, uploaded files, database files
etc)-

Anonymous Volumes can be useful for ensuring that some Container-internal folder is **not overwritten** by a "Bind Mount" for example.

By default, **Anonymous Volumes are removed** if the Container was started with the `--rm` option
and was stopped thereafter. They are not removed if a Container was started (and then
removed) without that option.

**Named Volumes are never removed**, you need to do that manually (via `docker volume rm VOL_NAME` , see reference below)

<br>

#### Bind Mounts

Bind Mounts are very similiar to Volumes - the key difference is, that you, the developer, **set the path on your host machine** that should be connected to some path inside of a Container.

You do that via  
`-v /absolute/path/on/your/host/machine:/some/path/inside/of/container` .

The path in front of the : (i.e. the path on your host machine, to the folder that should be shared with the container) has to be an absolute path when using `-v` on the `docker run` command.

Bind Mounts are very useful for **sharing data with a Container** which might change whilst the container is running - e.g. your source code that you want to share with the Container running your development environment.

**Don't use Bind Mounts if you just want to persist data** - Named Volumes should be used for that (exception: You want to be able to inspect the data written during development).

In general, **Bind Mounts are a great tool during development** - they're not meant to be used in production (since you're container should run isolated from it's host machine).

<br>

#### Key Docker Commands

- `docker run -v /path/in/container IMAGE` : Create an **Anonymous Volume** inside a Container

- `docker run -v some-name:/path/in/container IMAGE` : Create a **Named Volume** (named some-name ) inside a Container

- `docker run -v /path/on/your/host/machine:path/in/container IMAGE` : Create a **Bind Mount** and connect a local path on your host machine to some path in the Container

- `docker volume ls` : **List all currently active / stored Volumes** (by all Containers)

- `docker volume create VOL_NAME` : **Create a new (Named) Volume** named `VOL_NAME` . You typically don't need to do that, since Docker creates them automatically for you if they don't exist when running a container

- `docker volume rm VOL_NAME` : **Remove a Volume** by it's name (or ID)

- `docker volume prune` : **Remove all unused Volumes** (i.e. not connected to a currently running or stopped container)

<br>

---

<br>

### Networking: (Cross-)Container Communication

- **Docker Network Drivers**

  - Docker Networks actually support different kinds of "**Drivers**" which influence the behavior of the Network.

  - The default driver is the "**bridge**" driver - it provides the behavior shown in this module (i.e. Containers can find each other by name if they are in the same Network).

  - The driver can be set when a Network is created, simply by adding the `--driver` option.

    - ```
      docker network create --driver bridge my-net
      ```

  - Of course, if you want to use the "bridge" driver, you can simply omit the entire option since "bridge" is the default anyways.

  - Docker also supports these alternative drivers - though you will use the "bridge" driver in most cases:

    - **host**: For standalone containers, isolation between container and host system is removed (i.e. they share localhost as a network)

    - **overlay**: Multiple Docker daemons (i.e. Docker running on different machines) are able to connect with each other. Only works in "Swarm" mode which is a dated / almost deprecated way of connecting multiple containers

    - **macvlan**: You can set a custom MAC address to a container - this address can then be used for communication with that container

    - **none**: All networking is disabled.

    - **Third-party plugins**: You can install third-party plugins which then may add all kinds of behaviors and functionalities

  As mentioned, the "**bridge**" driver makes most sense in the vast majority of scenarios.

<br>

### Cheat Sheet Networks Request

<br>

#### Network / Requests

In many application, you'll need more than one container - for **2 main reasons**:

1. It's considered a **good practice** to focus each container on **one main task** (e.g. run a web server, run a database, ...)
2. It's **very hard** to configure a Container that **does more than one "main thing"** (e.g. run a web server AND a database)

Multi-Container apps are quite common, espcially if you're working on "real applications".

Often, some of these need to communicate though:

- either **with each other**
- or with the **host machine**
- or with the **world wide web**

<br>

#### Communicatiing with the World Wide Web (WWW)

Communicating with the WWW (i.e. sending Http request or other kinds of requests to other servers) is thankfully very easy.

Consider this JavaScript example - though it'll always work, no matter which technology you're using

```
fetch('https://some-api.com/my-data').then(...)
```

This very basic code snippet tries to send a `GET` request to `some-api.com/my-data` .

This will **work out of the box**, no extra configuration is required! The application, running in a Container, will have no problems sending this request.

<br>

#### Communicating with Host Machine

Communicating with the Host Machine (e.g. because you have a database running on the Host Machine) is also quite simple, though it **doesn't work without any changes**.

**One important note**: If you deploy a Container onto a server (i.e. another machine), it's very unlikely that you'll need to communicate with that machine. Communicating to the Host Machine typically is a requirement during development - for example because you're running some development database on your machine.

Again, consider this JS example:

```
fetch('localhost:3000/demo').then(...)
```

This code snippet tries to send a GET request to some web server running on the local host machine (i.e. **outside** of the Container but not the WWW).

On your local machine, this would work - inside of a Container, it **will fail**. Because `localhost` inside of the Container refers to the Container environment, **not to your local host machine which is running the Container / Docker**!

But Docker has got you covered!

You just need to change this snippet like this:

```
fetch('host.docker.internal:3000/demo').then(...)
```

`host.docker.internal` is a special address / identifier which is translated to the IP address of the machine hosting the Container by Docker.

**Important**: "Translated" does **not** mean that Docker goes ahead and changes the source code. Instead, it simply detects the outgoing request and is able to resolve the IP address for that request.

<br>

#### Communicating with Other Containers

Communicating with other Containers is also quite straightforward. You have two main options:

1. **Manually find out the IP** of the other Container (it may change though)
2. Use **Docker Networks** and put the communicating Containers into the same Network

Option 1 is not great since you need to search for the IP on your own and it might change over
time.

Option 2 is perfect though. With Docker, you can create Networks via `docker network create SOME_NAME` and you can then attach multiple Containers to one and the same Network.

Like this:

```
docker run -network my-network --name cont1 my-image
docker run -network my-network --name cont2 my-other-image
```

Both `cont1` and `cont2` will be in the same Network.

Now, you can simply **use the Container names** to let them communicate with each other - again, Docker will resolve the IP for you (see above)

```
fetch('cont1/my-data').then(...)
```

<br>

## Dockerize a Django Application

- [Docker | How to Dockerize a Django application (Beginners Guide)](https://www.youtube.com/watch?v=W5Ov0H7E_o4&ab_channel=VeryAcademy)

<br>

# References üåê

1. [Docker & Kubernetes: The Practical Guide [2022 Edition]](https://www.udemy.com/course/docker-kubernetes-the-practical-guide/)
2. [Introduction to Microservices, Docker, and Kubernetes](https://www.youtube.com/watch?v=1xo-0gCVhTU&ab_channel=JamesQuigley)
